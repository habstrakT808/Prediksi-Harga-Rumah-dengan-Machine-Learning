# -*- coding: utf-8 -*-
"""PREDIKSI HARGA RUMAH_HAFIYAN AL MUQAFFI UMARY.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZqzAvM3nbqPl2Y7Hv2oyoX3ADX8voqEf

#PREDIKSI HARGA RUMAH - MACHINE LEARNING TERAPAN

## Import library yang dibutuhkan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

"""## 1. Load Dataset

"""

print("Loading dataset...")
url = "https://www.kaggle.com/datasets/harlfoxem/housesalesprediction"
df = pd.read_csv("/content/kc_house_data.csv")

"""## 2. Exploratory Data Analysis"""

print("\n=== Data Overview ===")
print(f"Shape: {df.shape}")
print("\nData Types:")
print(df.dtypes)

print("\n=== Statistical Summary ===")
print(df.describe())

print("\n=== Check Missing Values ===")
print(df.isnull().sum())

# Convert date column to datetime
df['date'] = pd.to_datetime(df['date'])

# Visualizations
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(df['price'], kde=True)
plt.title('Price Distribution')
plt.xlabel('Price')

plt.subplot(1, 2, 2)
sns.scatterplot(x='sqft_living', y='price', data=df)
plt.title('Price vs. Square Footage')
plt.tight_layout()
plt.savefig('price_distribution_and_correlation.png')
plt.close()

# Correlation heatmap
plt.figure(figsize=(14, 12))
correlation = df.corr()
mask = np.triu(correlation)
sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt='.2f', mask=mask)
plt.title('Correlation Heatmap')
plt.tight_layout()
plt.savefig('correlation_heatmap.png')
plt.close()

print("\n=== Top Correlations with Price ===")
print(correlation['price'].sort_values(ascending=False).head(10))

# Plot locations with prices
plt.figure(figsize=(12, 8))
sns.scatterplot(x='long', y='lat', data=df, hue='price', palette='viridis', size='price',
               sizes=(20, 200), alpha=0.6)
plt.title('House Locations with Prices')
plt.tight_layout()
plt.savefig('location_prices.png')
plt.close()

"""## 3. Data Preparation

### 3.1 Handling Missing Values
"""

print("Handling missing values...")
df['waterfront'] = df['waterfront'].fillna(0)
df['view'] = df['view'].fillna(0)
df['yr_renovated'] = df['yr_renovated'].fillna(0)

"""### 3.2 Feature Engineering"""

print("Creating new features...")
# Age of the house
df['age'] = 2015 - df['yr_built']

# Renovation status
df['renovated'] = df['yr_renovated'].apply(lambda x: 1 if x > 0 else 0)

# Total area
df['total_area'] = df['sqft_living'] + df['sqft_lot']

# Price per square foot
df['price_per_sqft'] = df['price'] / df['sqft_living']

# Extract date features
df['sale_year'] = df['date'].dt.year
df['sale_month'] = df['date'].dt.month
df['sale_day'] = df['date'].dt.day

"""### 3.3 Handling Outliers"""

print("Handling outliers...")
# Visualize price outliers with boxplot
plt.figure(figsize=(10, 6))
sns.boxplot(x=df['price'])
plt.title('Price Boxplot')
plt.tight_layout()
plt.savefig('price_boxplot.png')
plt.close()

# Remove price outliers using IQR method
Q1 = df['price'].quantile(0.25)
Q3 = df['price'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

df_clean = df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound)]
print(f"Rows after removing outliers: {df_clean.shape[0]} (removed {df.shape[0] - df_clean.shape[0]} rows)")

"""### 3.4 Feature Selection based on correlation"""

print("Selecting features based on correlation...")
corr_with_target = df_clean.corr()['price'].abs().sort_values(ascending=False)
print("\nTop 15 features by correlation with price:")
print(corr_with_target.head(16))

top_features = corr_with_target[1:16].index  # Excluding price itself

"""### 3.5 Feature Scaling"""

print("\nApplying feature scaling...")
numeric_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',
                   'sqft_above', 'sqft_basement', 'age', 'total_area']

scaler = StandardScaler()
df_clean[numeric_features] = scaler.fit_transform(df_clean[numeric_features])

"""### 3.6 One-Hot Encoding for zipcode"""

print("Applying one-hot encoding for zipcode...")
df_encoded = pd.get_dummies(df_clean, columns=['zipcode'], drop_first=True)

# Final dataset for modeling
selected_features = list(top_features)
# Add engineered features if not already in selected_features
for feature in ['renovated', 'age', 'total_area', 'sale_month']:
    if feature not in selected_features:
        selected_features.append(feature)

# Add some zipcode columns (location matters for house prices)
zipcode_cols = [col for col in df_encoded.columns if 'zipcode' in col][:10]  # Take first 10 zipcodes
selected_features.extend(zipcode_cols)

# Prepare final dataframe with selected features
df_final = df_encoded[['price'] + selected_features].copy()

print(f"\nFinal dataset shape: {df_final.shape}")
print("Selected features:", selected_features)

"""## 4. Modeling

### 4.1 Split data into training and testing sets
"""

X = df_final[selected_features]
y = df_final['price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Training set: {X_train.shape}")
print(f"Testing set: {X_test.shape}")

"""### 4.2 Function to evaluate models"""

def evaluate_model(y_true, y_pred, model_name):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)

    print(f"\nModel: {model_name}")
    print(f"MAE: ${mae:.2f}")
    print(f"MSE: ${mse:.2f}")
    print(f"RMSE: ${rmse:.2f}")
    print(f"R²: {r2:.4f}")
    print("-" * 50)

    return mae, mse, rmse, r2

"""### 4.3 Model 1: Linear Regression"""

print("Training Linear Regression model...")
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Make predictions
y_pred_lr = lr_model.predict(X_test)

# Evaluate model
lr_metrics = evaluate_model(y_test, y_pred_lr, "Linear Regression")

"""### 4.4 Model 2: Decision Tree Regressor"""

print("Training Decision Tree model...")
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)

# Make predictions
y_pred_dt = dt_model.predict(X_test)

# Evaluate model
dt_metrics = evaluate_model(y_test, y_pred_dt, "Decision Tree")

"""### 4.5 Model 3: Random Forest Regressor"""

print("Training Random Forest model...")
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf_model.predict(X_test)

# Evaluate model
rf_metrics = evaluate_model(y_test, y_pred_rf, "Random Forest")

"""### 4.6 Hyperparameter Tuning for Random Forest"""

print("Performing hyperparameter tuning for Random Forest...")
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# For quick execution, we'll use a smaller parameter grid
# Uncomment the below code and comment the above for a more comprehensive search
# param_grid = {
#     'n_estimators': [100],
#     'max_depth': [None, 20],
#     'min_samples_split': [2, 5],
#     'min_samples_leaf': [1, 2]
# }

grid_search = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid,
    cv=3,  # Use 3-fold cross-validation
    scoring='neg_mean_squared_error',
    n_jobs=-1  # Use all available cores
)

grid_search.fit(X_train, y_train)

# Best parameters
print("\nBest parameters:", grid_search.best_params_)

# Best model
best_rf_model = grid_search.best_estimator_
y_pred_best_rf = best_rf_model.predict(X_test)

# Evaluate tuned model
best_rf_metrics = evaluate_model(y_test, y_pred_best_rf, "Random Forest (Tuned)")

"""### 4.7 Feature Importance"""

print("\n=== Feature Importance from Random Forest Model ===")
feature_importance = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': best_rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

print(feature_importance.head(10))

# Visualize feature importance
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))
plt.title('Top 15 Features by Importance')
plt.tight_layout()
plt.savefig('feature_importance.png')
plt.close()

"""## 5. Model Comparison"""

print("\n=== Model Comparison ===")
models = ['Linear Regression', 'Decision Tree', 'Random Forest', 'Random Forest (Tuned)']
metrics = [lr_metrics, dt_metrics, rf_metrics, best_rf_metrics]

mae_values = [metric[0] for metric in metrics]
rmse_values = [metric[2] for metric in metrics]
r2_values = [metric[3] for metric in metrics]

# Create comparison plots
plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
sns.barplot(x=models, y=mae_values)
plt.title('MAE Comparison')
plt.ylabel('MAE ($)')
plt.xticks(rotation=45)

plt.subplot(2, 2, 2)
sns.barplot(x=models, y=rmse_values)
plt.title('RMSE Comparison')
plt.ylabel('RMSE ($)')
plt.xticks(rotation=45)

plt.subplot(2, 2, 3)
sns.barplot(x=models, y=r2_values)
plt.title('R² Comparison')
plt.ylabel('R²')
plt.xticks(rotation=45)

plt.subplot(2, 2, 4)
# Actual vs Predicted plot for best model
plt.scatter(y_test, y_pred_best_rf, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Prices (Random Forest Tuned)')

plt.tight_layout()
plt.savefig('model_comparison.png')
plt.close()

print("\n=== Best Model: Random Forest (Tuned) ===")
print(f"MAE: ${best_rf_metrics[0]:.2f}")
print(f"RMSE: ${best_rf_metrics[2]:.2f}")
print(f"R²: {best_rf_metrics[3]:.4f}")

"""## 6. Save the best model

"""

print("\nSaving the best model...")
import joblib
joblib.dump(best_rf_model, 'house_price_prediction_model.pkl')
print("Model saved as 'house_price_prediction_model.pkl'")

"""## 7. Example prediction with the best model"""

print("\n=== Example Prediction ===")
# Create a sample house
sample_house = X_test.iloc[0].copy()
actual_price = y_test.iloc[0]

# Make prediction
predicted_price = best_rf_model.predict([sample_house])[0]

print(f"Actual price: ${actual_price:.2f}")
print(f"Predicted price: ${predicted_price:.2f}")
print(f"Difference: ${abs(actual_price - predicted_price):.2f}")
print(f"Percentage Error: {abs(actual_price - predicted_price) / actual_price * 100:.2f}%")

print("\n=== Project Completed Successfully ===")